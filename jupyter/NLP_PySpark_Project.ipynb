{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42af0926",
   "metadata": {},
   "source": [
    "# Análise de Sentimentos de Avaliações do IMDB \n",
    "\n",
    "Os scripts abaixo realizam a limpeza e transformação dos dados. Além disso, são criados vários modelos de machine learning, com o objetivo de prever se o texto de avalição de um filme foi postivo ou negativo.\n",
    "\n",
    "Por último, todos o desempenho de todos os modelos são comparados, com base na acurácia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dbbc70",
   "metadata": {},
   "source": [
    "## Importando Bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e290ae49",
   "metadata": {},
   "source": [
    "### Criação da Instância Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5da8f412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1b3fd4ec2e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criação da instância Spark\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NLP\").config(\"spark.executor.heartbeatInterval\",\"700000s\") \\\n",
    ".config(\"spark.network.timeout\", \"900000s\").config(\"spark.driver.memory\", \"8g\").config(\"spark.memory.offHeap.enabled\",\"true\") \\\n",
    ".config(\"spark.memory.offHeap.size\",\"10g\").config(\"spark.executor.memory\",\"6g\") \\\n",
    ".config(\"spark.task.cpus\", 2).config(\"spark.executor.cores\",\"3\").getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf194be",
   "metadata": {},
   "source": [
    "### Importação de Outras Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47838eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml import Pipeline \n",
    "from pyspark.ml.feature import * #CountVectorizer,StringIndexer, RegexTokenizer,StopWordsRemover\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.functions import * #col, udf,regexp_replace,isnull\n",
    "from pyspark.sql.types import StringType,IntegerType\n",
    "\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b079838f",
   "metadata": {},
   "source": [
    "### Importando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f4eb37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/dados_brutos/\"\n",
    "\n",
    "# CSV\n",
    "reviews = spark.read.csv(path+'IMDB Dataset.csv',header=True, escape=\"\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740bc60d",
   "metadata": {},
   "source": [
    "### Verificando características dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e48dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f56784",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b469e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Número de avaliações: \", reviews.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84308261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|              review|sentiment|\n",
      "+--------------------+---------+\n",
      "|One of the other ...| positive|\n",
      "|A wonderful littl...| positive|\n",
      "+--------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(reviews.show(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d946eef8",
   "metadata": {},
   "source": [
    "##### Verifica se existe algum valor nulo entre os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f686a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def null_value_calc(df):\n",
    "    null_columns_counts = []\n",
    "    numRows = df.count()\n",
    "    for k in df.columns:\n",
    "        nullRows = df.where(col(k).isNull()).count()\n",
    "        if(nullRows > 0):\n",
    "            temp = k,nullRows,(nullRows/numRows)*100\n",
    "            null_columns_counts.append(temp)\n",
    "    return(null_columns_counts)\n",
    "\n",
    "null_columns_calc_list = null_value_calc(reviews)\n",
    "\n",
    "if (len(null_columns_calc_list) > 0):\n",
    "    spark.createDataFrame(null_columns_calc_list, ['Column_Name', 'Null_Values_Count','Null_Value_Percent']).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0ed937",
   "metadata": {},
   "source": [
    "##### Como não existe nenhuma linha de dados nula, não é necessário nenhum tratamento adicional. \n",
    "\n",
    "##### Verifica o balanceamento entre as classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7220c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.groupBy(\"sentiment\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7539418",
   "metadata": {},
   "source": [
    "##### Como as classes estão balanceadas, não é necessário nenhum tratamento adicional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573d1e81",
   "metadata": {},
   "source": [
    "### Transformação dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc5c302",
   "metadata": {},
   "source": [
    "##### Transforma os textos usados na coluna \"sentiment\" em labels númericos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0931be85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reviews\n",
    "indexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n",
    "df = indexer.fit(df).transform(df)\n",
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d7ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove as tags das avaliações\n",
    "df = df.withColumn(\"review\",regexp_replace(df[\"review\"], '<.*/>', ''))\n",
    "# Remove tudo que não seja uma letra\n",
    "df = df.withColumn(\"review\",regexp_replace(df[\"review\"], '[^A-Za-z ]+', ''))\n",
    "# Remove múltiplos espaços em branco\n",
    "df = df.withColumn(\"review\",regexp_replace(df[\"review\"], ' +', ' '))\n",
    "# Coloca todo o texto em minúsculo\n",
    "df = df.withColumn(\"review\",lower(df[\"review\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82efb319",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b223d0",
   "metadata": {},
   "source": [
    "##### Cria coluna \"words\" que contém cada palavra do texto completo da avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8019cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol=\"review\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "df = regex_tokenizer.transform(df)\n",
    "\n",
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bc1347",
   "metadata": {},
   "source": [
    "#####  Exclui todas as palavras que são consideradas irrelevantes para o entedimento do sentido de um texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46bd21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "feature_data = remover.transform(df)\n",
    "    \n",
    "feature_data.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577315e5",
   "metadata": {},
   "source": [
    "##### Cria coluna \"rawfeatures\" que contém tupla com 3 itens. O primeiro item é a quantidade de grupos de palavras passados como parâmetro. O segundo item representa o índice da palavra. Como as palavras são agrupadas, o índice representa, o grupo onde a palavra foi inserido. O terceiro item é a frequência absoluta de cada palavra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114bd263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existe a Count Vector que faz a mesma coisa que a HashingTF, porém a quantidade de grupos será igual a quantidade total de palavras\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawfeatures\", numFeatures=250)\n",
    "HTFfeaturizedData = hashingTF.transform(feature_data)\n",
    "\n",
    "HTFfeaturizedData.limit(10).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c5da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTFfeaturizedData.show(3,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5208e5f",
   "metadata": {},
   "source": [
    "##### Cria coluna \"features\" que contém tupla com 3 itens. O retorno é bem parecido com a HashingTF, porém, aqui o terceito item será o resultado do seguinte cálculo:\n",
    "##### idf = log((m + 1) / (d(t) + 1)), onde m é o número total de linhas e d(t) é o número de linhas que contém a palavra t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99047860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "idf = IDF(inputCol=\"rawfeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(HTFfeaturizedData)\n",
    "TFIDFfeaturizedData = idfModel.transform(HTFfeaturizedData)\n",
    "TFIDFfeaturizedData.name = 'TFIDFfeaturizedData'\n",
    "\n",
    "# Renomeia coluna \"rawfetatures\" para \"features\" para consistência\n",
    "HTFfeaturizedData = HTFfeaturizedData.withColumnRenamed(\"rawfeatures\",\"features\")\n",
    "HTFfeaturizedData.name = 'HTFfeaturizedData' #We will use later for printing\n",
    "\n",
    "if os.path.isdir(\"../data/dados_transformados/HTFfeaturizedData\"):\n",
    "    HTFfeaturizedData.write.mode(\"Overwrite\").partitionBy(\"label\").parquet(\"../data/dados_transformados/HTFfeaturizedData\")\n",
    "else:\n",
    "    HTFfeaturizedData.write.partitionBy(\"label\").parquet(\"../data/dados_transformados/HTFfeaturizedData\")\n",
    "    \n",
    "if os.path.isdir(\"../data/dados_transformados/TFIDFfeaturizedData\"):\n",
    "    TFIDFfeaturizedData.write.mode(\"Overwrite\").partitionBy(\"label\").parquet(\"../data/dados_transformados/TFIDFfeaturizedData\")\n",
    "else:\n",
    "    TFIDFfeaturizedData.write.partitionBy(\"label\").parquet(\"../data/dados_transformados/TFIDFfeaturizedData\")\n",
    "\n",
    "TFIDFfeaturizedData.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a422adee",
   "metadata": {},
   "source": [
    "##### As tabelas HTFfeaturizedData e TFIDFfeaturizedData serão usadas na construção de difentes modelos de machine learning. Os dados dessas duas tabelas são salvos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6303f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDFfeaturizedData.show(3,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de9daae",
   "metadata": {},
   "source": [
    "##### O Word2Vec cria a coluna \"filtered\" que retorna uma lista de valores baseada na lista de palavras. O parâmetro \"vectorSize\" determina o número de valores da lista e o número de grupos , e o \"minCount\" determina a quantidade mínima de aparições que uma palavra deve ter para ser considerada no cálculo. Logo, cada valor da lista mostra o grau de similaridade da lista de palavras com cada um dos grupos\n",
    "\n",
    "##### O Word2Vec retorna alguns valores negativos. Para corrigir isso, o MinMaxScaler  fará o seguinte cálculo:\n",
    "##### Formula = ((VALOR-COLUNA_MIN)/(COLUNA_MAX-COLUNA_MIN)) *(max - min) + min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930de6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "word2Vec = Word2Vec(vectorSize=250, minCount=5, inputCol=\"filtered\", outputCol=\"features\")\n",
    "model = word2Vec.fit(feature_data)\n",
    "\n",
    "W2VfeaturizedData = model.transform(feature_data)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "scalerModel = scaler.fit(W2VfeaturizedData)\n",
    "\n",
    "scaled_data = scalerModel.transform(W2VfeaturizedData)\n",
    "W2VfeaturizedData = scaled_data.select('sentiment','review','label','scaledFeatures')\n",
    "# Renomeia coluna \"scaledFeatures\" para \"features\" para consistência\n",
    "W2VfeaturizedData = W2VfeaturizedData.withColumnRenamed('scaledFeatures','features')\n",
    "\n",
    "W2VfeaturizedData.name = 'W2VfeaturizedData'\n",
    "\n",
    "if os.path.isdir(\"../data/dados_transformados/W2VfeaturizedData\"):\n",
    "    W2VfeaturizedData.write.mode(\"Overwrite\").partitionBy(\"label\").parquet(\"../data/dados_transformados/W2VfeaturizedData\")\n",
    "else:\n",
    "    W2VfeaturizedData.write.partitionBy(\"label\").parquet(\"../data/dados_transformados/W2VfeaturizedData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ea060",
   "metadata": {},
   "source": [
    "##### A tabela W2VfeaturizedData será usada na construção de difentes modelos de machine learning. Os dados dessa tabela são salvos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2VfeaturizedData.show(3,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a3bd9",
   "metadata": {},
   "source": [
    "### Criação dos modelos de machine learning\n",
    "\n",
    "Serão testados 3 tabelas e um conjunto de várias técnicas de classificação. Cada combinação irá gerar um modelo de machine learning que serão comparados pela acurácia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3330e6d0",
   "metadata": {},
   "source": [
    "##### A função abaixo fará a criação do modelo de machine learning para cada combinação de tabelas e tipos de modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f7abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClassTrainEval(classifier,features,classes,train,test):\n",
    "\n",
    "    def FindMtype(classifier):\n",
    "        M = classifier\n",
    "        # Retorna texto com o tipo de classificador usado\n",
    "        Mtype = type(M).__name__\n",
    "        \n",
    "        return Mtype\n",
    "    \n",
    "    Mtype = FindMtype(classifier)\n",
    "    \n",
    "\n",
    "    def IntanceFitModel(Mtype,classifier,classes,features,train):\n",
    "        \n",
    "        if Mtype == \"OneVsRest\":\n",
    "            # Instância classificador\n",
    "            lr = LogisticRegression()\n",
    "            # Cria instância do classificador OneVsRest\n",
    "            OVRclassifier = OneVsRest(classifier=lr)\n",
    "            # Adiciona parâmetros de livre escolha\n",
    "            paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "                .build()\n",
    "            # Usa o CrossValidator para validar o melhor modelo dentre a lista de parâmetros escolhidos\n",
    "            crossval = CrossValidator(estimator=OVRclassifier,\n",
    "                                      estimatorParamMaps=paramGrid,\n",
    "                                      evaluator=MulticlassClassificationEvaluator(),\n",
    "                                      numFolds=2)\n",
    "            fitModel = crossval.fit(train)\n",
    "            return fitModel\n",
    "        if Mtype == \"MultilayerPerceptronClassifier\":\n",
    "            # Especifica camadas da rede neural\n",
    "            # Camada de entrada terá o tamanho igual a quantidade de valores da coluna \"features\"\n",
    "            # As camadas intermediárias terão o mesmo tamanho mais 1 e o mesmo tamanho usado na camada de entrada\n",
    "            # A camada de saída terá o tamanho igual ao número de classes\n",
    "            # O CrossValidator não é usado aqui\n",
    "            features_count = len(features[0][0])\n",
    "            layers = [features_count, features_count+1, features_count, classes]\n",
    "            MPC_classifier = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "            fitModel = MPC_classifier.fit(train)\n",
    "            return fitModel\n",
    "        if Mtype in(\"LogisticRegression\",\"NaiveBayes\",\"RandomForestClassifier\",\"GBTClassifier\",\"LinearSVC\",\"DecisionTreeClassifier\"):\n",
    "  \n",
    "            # Adiciona parâmetros de livre escolha\n",
    "            if Mtype in(\"LogisticRegression\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "#                              .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
    "                             .addGrid(classifier.maxIter, [10, 15,20])\n",
    "                             .build())\n",
    "                \n",
    "            # Adiciona parâmetros de livre escolha\n",
    "            if Mtype in(\"NaiveBayes\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                             .addGrid(classifier.smoothing, [0.0, 0.2, 0.4, 0.6]) \\\n",
    "                             .build())\n",
    "                \n",
    "            # Adiciona parâmetros de livre escolha\n",
    "            if Mtype in(\"RandomForestClassifier\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                               .addGrid(classifier.maxDepth, [2, 5, 10])\n",
    "#                                .addGrid(classifier.maxBins, [5, 10, 20])\n",
    "#                                .addGrid(classifier.numTrees, [5, 20, 50])\n",
    "                             .build())\n",
    "                \n",
    "            # Adiciona parâmetros de livre escolha\n",
    "            if Mtype in(\"GBTClassifier\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "#                              .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "#                              .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
    "                             .addGrid(classifier.maxIter, [10, 15,50,100])\n",
    "                             .build())\n",
    "                \n",
    "           # Adiciona parâmetros de livre escolha\n",
    "            if Mtype in(\"LinearSVC\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "                             .addGrid(classifier.maxIter, [10, 15]) \\\n",
    "                             .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
    "                             .build())\n",
    "            \n",
    "            # Adiciona parâmetros de livre escolha\n",
    "            if Mtype in(\"DecisionTreeClassifier\"):\n",
    "                paramGrid = (ParamGridBuilder() \\\n",
    "#                              .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "                             .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
    "                             .build())\n",
    "            \n",
    "            # Usa o CrossValidator para validar o melhor modelo dentre a lista de parâmetros escolhidos\n",
    "            crossval = CrossValidator(estimator=classifier,\n",
    "                                      estimatorParamMaps=paramGrid,\n",
    "                                      evaluator=MulticlassClassificationEvaluator(),\n",
    "                                      numFolds=2)\n",
    "            fitModel = crossval.fit(train)\n",
    "            return fitModel\n",
    "    \n",
    "    fitModel = IntanceFitModel(Mtype,classifier,classes,features,train)\n",
    "    \n",
    "    # Mostra na tela alguma métricas de cada tipo de classificador\n",
    "    if fitModel is not None:\n",
    "        \n",
    "        if Mtype in(\"OneVsRest\"):\n",
    "            # Obtém o melhor modelo do CrossValidator\n",
    "            BestModel = fitModel.bestModel\n",
    "            print(\" \")\n",
    "            print('\\033[1m' + Mtype + '\\033[0m')\n",
    "            # Extraí a lista de coeficientes e a interseção do modelo\n",
    "            models = BestModel.models\n",
    "            for model in models:\n",
    "                print('\\033[1m' + 'Interseção: '+ '\\033[0m',model.intercept,'\\033[1m' + '\\nCoeficientes:'+ '\\033[0m',model.coefficients)\n",
    "\n",
    "        if Mtype == \"MultilayerPerceptronClassifier\":\n",
    "            print(\"\")\n",
    "            print('\\033[1m' + Mtype,\" Pesos\"+ '\\033[0m')\n",
    "            print('\\033[1m' + \"Pesos do Modelo: \"+ '\\033[0m',fitModel.weights.size)\n",
    "            print(\"\")\n",
    "\n",
    "        if Mtype in(\"DecisionTreeClassifier\", \"GBTClassifier\",\"RandomForestClassifier\"):\n",
    "            # FEATURE IMPORTANCES\n",
    "            # Obtém o melhor modelo do CrossValidator\n",
    "            BestModel = fitModel.bestModel\n",
    "            print(\" \")\n",
    "            print('\\033[1m' + Mtype,\" Importância das entradas\"+ '\\033[0m')\n",
    "            print(\"(A pontuação mais próxima de 1 é a mais importante)\")\n",
    "            print(\"Pontuação mais baixa é a menos importante\")\n",
    "            print(\" \")\n",
    "            print(BestModel.featureImportances)\n",
    "            \n",
    "            if Mtype in(\"DecisionTreeClassifier\"):\n",
    "                global DT_featureimportances\n",
    "                DT_featureimportances = BestModel.featureImportances.toArray()\n",
    "                global DT_BestModel\n",
    "                DT_BestModel = BestModel\n",
    "            if Mtype in(\"GBTClassifier\"):\n",
    "                global GBT_featureimportances\n",
    "                GBT_featureimportances = BestModel.featureImportances.toArray()\n",
    "                global GBT_BestModel\n",
    "                GBT_BestModel = BestModel\n",
    "            if Mtype in(\"RandomForestClassifier\"):\n",
    "                global RF_featureimportances\n",
    "                RF_featureimportances = BestModel.featureImportances.toArray()\n",
    "                global RF_BestModel\n",
    "                RF_BestModel = BestModel\n",
    "\n",
    "        if Mtype in(\"LogisticRegression\"):\n",
    "            # Obtém o melhor modelo do CrossValidator\n",
    "            BestModel = fitModel.bestModel\n",
    "            print(\" \")\n",
    "            print('\\033[1m' + Mtype,\" Matrix de coeficientes\"+ '\\033[0m')\n",
    "            print(\"Coeficientes: \\n\" + str(BestModel.coefficientMatrix))\n",
    "            print(\"Interseção: \" + str(BestModel.interceptVector))\n",
    "            global LR_coefficients\n",
    "            LR_coefficients = BestModel.coefficientMatrix.toArray()\n",
    "            global LR_BestModel\n",
    "            LR_BestModel = BestModel\n",
    "\n",
    "        if Mtype in(\"LinearSVC\"):\n",
    "            # Obtém o melhor modelo do CrossValidator\n",
    "            BestModel = fitModel.bestModel\n",
    "            print(\" \")\n",
    "            print('\\033[1m' + Mtype,\" Coeficientes\"+ '\\033[0m')\n",
    "            print(\"Coeficientes: \\n\" + str(BestModel.coefficients))\n",
    "            global LSVC_coefficients\n",
    "            LSVC_coefficients = BestModel.coefficients.toArray()\n",
    "            global LSVC_BestModel\n",
    "            LSVC_BestModel = BestModel\n",
    "        \n",
    "   \n",
    "    # Estabelece colunas da tabela que irá comparar os resultados de cada classificador\n",
    "    columns = ['Classifier', 'Result']\n",
    "    \n",
    "    if Mtype in(\"LinearSVC\",\"GBTClassifier\") and classes != 2:\n",
    "        Mtype = [Mtype] \n",
    "        score = [\"N/A\"]\n",
    "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
    "    else:\n",
    "        predictions = fitModel.transform(test)\n",
    "        # Avalia o modelo pela acurácia\n",
    "        MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "        accuracy = (MC_evaluator.evaluate(predictions))*100\n",
    "        Mtype = [Mtype]\n",
    "        score = [str(accuracy)]\n",
    "        # Insere na tabela os resultados do modelo\n",
    "        result = spark.createDataFrame(zip(Mtype,score), schema=columns)\n",
    "        result = result.withColumn('Result',result.Result.substr(0, 5))\n",
    "        # Salva modelo\n",
    "        if os.path.isdir('../output/' + Mtype[0] + '_' + train.name):\n",
    "            fitModel.write().overwrite().save('../output/' + Mtype[0] + '_' + train.name )\n",
    "        else:\n",
    "            fitModel.save('../output/' + Mtype[0] + '_' + train.name)\n",
    "        \n",
    "    return result\n",
    "    # Retorna tabela com os resultados do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34182be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifiers = [\n",
    "                LogisticRegression()\n",
    "                ,OneVsRest()\n",
    "               ,LinearSVC()\n",
    "               ,NaiveBayes()\n",
    "               ,RandomForestClassifier()\n",
    "               ,GBTClassifier()\n",
    "               ,DecisionTreeClassifier()\n",
    "               ,MultilayerPerceptronClassifier()\n",
    "              ] \n",
    "\n",
    "featureDF_list = [HTFfeaturizedData,TFIDFfeaturizedData,W2VfeaturizedData]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c98470",
   "metadata": {},
   "source": [
    "##### Para cada uma das 3 tabelas, cria modelos de machine learning beseados na lista de classificadores acima\n",
    "\n",
    "##### Cada modelo criado é avaliado pela acurácia. No final, uma tabela compara o resultado de cada modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e3449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for featureDF in featureDF_list:\n",
    "    print('\\033[1m' + featureDF.name,\" Results:\"+ '\\033[0m')\n",
    "    train, test = featureDF.randomSplit([0.7, 0.3],seed = 11)\n",
    "    train.name = featureDF.name\n",
    "    features = featureDF.select(['features']).collect()\n",
    "    # Retorna o número de classes\n",
    "    classes = featureDF.select(\"label\").distinct().count()\n",
    "\n",
    "    # Organiza tabela que receberá os resultados do modelo\n",
    "    columns = ['Classifier', 'Result']\n",
    "    vals = [(\"Place Holder\",\"N/A\")]\n",
    "    results = spark.createDataFrame(vals, columns)\n",
    "\n",
    "    for classifier in classifiers:\n",
    "        new_result = ClassTrainEval(classifier,features,classes,train,test)\n",
    "        results = results.union(new_result)\n",
    "    results = results.where(\"Classifier!='Place Holder'\")\n",
    "    # Mostra na tela a comparação do resultado de cada classificador\n",
    "    print(results.show(truncate=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
